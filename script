"""
Token Scanner - Multi-stage filtering system to find tokens before they explode
Combines DexScreener, GoPlus Security, LunarCrush, and Bubblemaps APIs
"""

import requests
import time
import json
from datetime import datetime, timedelta
from typing import Dict, List, Optional
import pandas as pd
from dataclasses import dataclass
import os


@dataclass
class TokenCandidate:
    """Data structure for token candidates"""
    address: str
    chain: str
    symbol: str
    name: str
    price_usd: float
    liquidity_usd: float
    volume_24h: float
    price_change_24h: float
    market_cap: float
    holders: int
    age_hours: float
    dex: str
    pair_address: str


class TokenScanner:
    """Main scanner class with multi-stage filtering"""
    
    def __init__(self, bubblemaps_key: str, lunarcrush_key: str):
        self.bubblemaps_key = bubblemaps_key
        self.lunarcrush_key = lunarcrush_key
        
        # Quota management
        self.daily_quota = 500  # query-seconds for Bubblemaps
        self.used_quota = 0
        self.quota_reset = datetime.now() + timedelta(days=1)
        
        # Results storage
        self.results = []
        self.scan_history = []
        
        # API endpoints
        self.goplus_url = "https://api.gopluslabs.io/api/v1/token_security"
        self.dexscreener_url = "https://api.dexscreener.com/latest/dex"
        self.lunarcrush_url = "https://lunarcrush.com/api4/public"
        self.bubblemaps_url = "https://api.bubblemaps.io/v1"  # Adjust based on actual endpoint
        
    def log(self, message: str, level: str = "INFO"):
        """Simple logging"""
        # Show DEBUG messages by default to help diagnose issues
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        print(f"[{timestamp}] [{level}] {message}")
    
    def check_quota(self):
        """Check if we have quota remaining"""
        if datetime.now() >= self.quota_reset:
            self.used_quota = 0
            self.quota_reset = datetime.now() + timedelta(days=1)
            self.log("Quota reset - new day started")
        
        remaining = self.daily_quota - self.used_quota
        self.log(f"Bubblemaps quota: {remaining:.1f} seconds remaining")
        return remaining > 0
    
    # ============================================================
    # STAGE 1: TOKEN DISCOVERY
    # ============================================================
    
    def discover_new_tokens(self, chains: List[str] = ['bsc', 'ethereum', 'solana']) -> List[TokenCandidate]:
        """Stage 1: Discover new tokens from multiple sources"""
        self.log("STAGE 1: Discovering new tokens from DEX pairs...")
        
        all_candidates = []
        
        # === SOURCE 1: GeckoTerminal New Pools (FREE & EXCELLENT) ===
        self.log("Fetching new pools from GeckoTerminal...")
        gecko_candidates = self._get_geckoterminal_new_pools(chains)
        all_candidates.extend(gecko_candidates)
        
        # === SOURCE 2: Boosted Tokens ===
        self.log("Fetching latest boosted tokens...")
        try:
            url = "https://api.dexscreener.com/token-boosts/latest/v1"
            response = requests.get(url, timeout=10)
            
            if response.status_code == 200:
                boosted = response.json()
                self.log(f"Found {len(boosted) if isinstance(boosted, list) else 1} boosted tokens")
                
                # Get pairs for each boosted token
                boosted_list = [boosted] if isinstance(boosted, dict) else boosted
                for token in boosted_list[:20]:  # Limit to 20 boosted tokens
                    token_address = token.get('tokenAddress')
                    chain_id = token.get('chainId')
                    
                    if token_address and chain_id in chains:
                        # Get full pair data for this token
                        pairs = self._get_token_pairs(token_address)
                        for pair in pairs:
                            if pair.get('chainId') in chains:
                                candidate = self._parse_dexscreener_pair(pair, pair.get('chainId'))
                                if candidate:
                                    all_candidates.append(candidate)
                        
                        time.sleep(0.5)
            else:
                self.log(f"Boosted tokens endpoint returned {response.status_code}", "WARNING")
        except Exception as e:
            self.log(f"Error fetching boosted tokens: {e}", "WARNING")
        
        # === SOURCE 3: Trending Search Terms ===
        trending_searches = [
            'pepe', 'doge', 'shib', 'inu', 'cat', 'moon', 'rocket', 'ai', 
            'bot', 'meme', 'coin', 'token', 'pump'
        ]
        
        for query in trending_searches[:8]:  # Limit searches
            try:
                url = f"https://api.dexscreener.com/latest/dex/search?q={query}"
                response = requests.get(url, timeout=10)
                
                if response.status_code == 200:
                    data = response.json()
                    pairs = data.get('pairs', [])
                    
                    if pairs:
                        # Only take top 3 pairs per search to avoid spam
                        for pair in pairs[:3]:
                            try:
                                chain_id = pair.get('chainId')
                                if chain_id in chains:
                                    # Only include pairs created in last 30 days
                                    pair_created = pair.get('pairCreatedAt', 0)
                                    if pair_created:
                                        age_hours = (time.time() * 1000 - pair_created) / (1000 * 60 * 60)
                                        if age_hours < 720:  # Less than 30 days old
                                            candidate = self._parse_dexscreener_pair(pair, chain_id)
                                            if candidate:
                                                all_candidates.append(candidate)
                            except Exception as e:
                                self.log(f"Error parsing pair: {e}", "DEBUG")
                
                time.sleep(0.5)  # Rate limiting
                
            except Exception as e:
                self.log(f"Error searching for {query}: {e}", "WARNING")
        
        # Remove duplicates based on token address
        seen = set()
        unique_candidates = []
        for candidate in all_candidates:
            if candidate.address not in seen:
                seen.add(candidate.address)
                unique_candidates.append(candidate)
        
        self.log(f"Total unique candidates discovered: {len(unique_candidates)}")
        return unique_candidates
    
    def _get_geckoterminal_new_pools(self, chains: List[str]) -> List[TokenCandidate]:
        """Get new pools from GeckoTerminal (FREE API)"""
        candidates = []
        
        # Map chain names to GeckoTerminal network IDs
        gecko_networks = {
            'ethereum': 'eth',
            'bsc': 'bsc',
            'solana': 'solana',
            'polygon': 'polygon_pos',
            'arbitrum': 'arbitrum',
            'base': 'base',
            'avalanche': 'avax'
        }
        
        for chain in chains:
            network = gecko_networks.get(chain.lower())
            if not network:
                continue
            
            try:
                url = f"https://api.geckoterminal.com/api/v2/networks/{network}/new_pools"
                response = requests.get(url, timeout=10)
                
                if response.status_code == 200:
                    data = response.json()
                    pools = data.get('data', [])
                    
                    self.log(f"Found {len(pools)} new pools on {chain}")
                    
                    for pool in pools:
                        try:
                            candidate = self._parse_geckoterminal_pool(pool, chain)
                            if candidate:
                                candidates.append(candidate)
                        except Exception as e:
                            self.log(f"Error parsing pool: {e}", "DEBUG")
                
                time.sleep(1)  # Rate limiting
                
            except Exception as e:
                self.log(f"Error fetching {chain} pools from GeckoTerminal: {e}", "WARNING")
        
        return candidates
    
    def _parse_geckoterminal_pool(self, pool: Dict, chain: str) -> Optional[TokenCandidate]:
        """Parse GeckoTerminal pool data into TokenCandidate"""
        try:
            attrs = pool.get('attributes', {})
            
            # Extract token address from relationship
            token_id = pool.get('relationships', {}).get('base_token', {}).get('data', {}).get('id', '')
            if not token_id:
                return None
            
            # GeckoTerminal ID format: "network_address"
            token_address = token_id.split('_')[1] if '_' in token_id else token_id
            
            # Extract data
            name = attrs.get('name', 'Unknown')
            symbol = name.split('/')[0] if '/' in name else 'UNKNOWN'
            
            price_usd = float(attrs.get('base_token_price_usd', 0) or 0)
            liquidity = float(attrs.get('reserve_in_usd', 0) or 0)
            
            # Volume data
            volume_data = attrs.get('volume_usd', {})
            volume_24h = float(volume_data.get('h24', 0) or 0)
            
            # Price change
            price_change_data = attrs.get('price_change_percentage', {})
            price_change = float(price_change_data.get('h24', 0) or 0)
            
            # Market cap
            market_cap = float(attrs.get('fdv_usd', 0) or 0)
            
            # Age
            pool_created = attrs.get('pool_created_at')
            if pool_created:
                from datetime import datetime
                created_time = datetime.fromisoformat(pool_created.replace('Z', '+00:00'))
                age_hours = (datetime.now().timestamp() - created_time.timestamp()) / 3600
            else:
                age_hours = 999
            
            return TokenCandidate(
                address=token_address,
                chain=chain,
                symbol=symbol,
                name=name,
                price_usd=price_usd,
                liquidity_usd=liquidity,
                volume_24h=volume_24h,
                price_change_24h=price_change,
                market_cap=market_cap,
                holders=0,
                age_hours=age_hours,
                dex='geckoterminal',
                pair_address=pool.get('id', '')
            )
        except Exception as e:
            self.log(f"Error parsing GeckoTerminal pool: {e}", "DEBUG")
            return None
    
    def _get_token_pairs(self, token_address: str) -> List[Dict]:
        """Get all pairs for a specific token address"""
        try:
            url = f"https://api.dexscreener.com/latest/dex/tokens/{token_address}"
            response = requests.get(url, timeout=10)
            
            if response.status_code == 200:
                data = response.json()
                return data.get('pairs', [])
            return []
        except Exception as e:
            self.log(f"Error fetching pairs for token {token_address}: {e}", "DEBUG")
            return []
    
    def _parse_dexscreener_pair(self, pair: Dict, chain: str) -> Optional[TokenCandidate]:
        """Parse DexScreener pair data into TokenCandidate"""
        try:
            base_token = pair.get('baseToken', {})
            
            # Extract data
            address = base_token.get('address', '')
            if not address:
                return None
                
            symbol = base_token.get('symbol', 'UNKNOWN')
            name = base_token.get('name', 'Unknown')
            
            price_usd = float(pair.get('priceUsd', 0))
            liquidity = float(pair.get('liquidity', {}).get('usd', 0))
            volume_24h = float(pair.get('volume', {}).get('h24', 0))
            price_change = float(pair.get('priceChange', {}).get('h24', 0))
            market_cap = float(pair.get('fdv', 0) or pair.get('marketCap', 0))
            
            # Calculate age
            pair_created = pair.get('pairCreatedAt')
            if pair_created:
                age_hours = (time.time() * 1000 - pair_created) / (1000 * 60 * 60)
            else:
                age_hours = 999  # Unknown age
            
            return TokenCandidate(
                address=address,
                chain=pair.get('chainId', chain),
                symbol=symbol,
                name=name,
                price_usd=price_usd,
                liquidity_usd=liquidity,
                volume_24h=volume_24h,
                price_change_24h=price_change,
                market_cap=market_cap,
                holders=0,  # Will get from other sources
                age_hours=age_hours,
                dex=pair.get('dexId', 'unknown'),
                pair_address=pair.get('pairAddress', '')
            )
        except Exception as e:
            self.log(f"Error parsing pair: {e}", "DEBUG")
            return None
    
    # ============================================================
    # STAGE 2: PRE-FILTERING
    # ============================================================
    
    def pre_filter_tokens(self, candidates: List[TokenCandidate]) -> List[TokenCandidate]:
        """Stage 2: Apply basic filters to eliminate obvious bad tokens"""
        self.log("STAGE 2: Pre-filtering tokens...")
        
        filtered = []
        
        for token in candidates:
            score = 0
            reasons = []
            
            # Liquidity check (more lenient for new tokens)
            if token.liquidity_usd < 10000:
                reasons.append(f"Low liquidity: ${token.liquidity_usd:,.0f}")
            elif token.liquidity_usd > 50000:
                score += 20
                reasons.append(f"Good liquidity: ${token.liquidity_usd:,.0f}")
            else:
                score += 10
                reasons.append(f"Moderate liquidity: ${token.liquidity_usd:,.0f}")
            
            # Volume check (more lenient)
            if token.volume_24h < 10000:
                reasons.append(f"Low volume: ${token.volume_24h:,.0f}")
            elif token.volume_24h > 50000:
                score += 20
                reasons.append(f"Good volume: ${token.volume_24h:,.0f}")
            else:
                score += 10
            
            # Price change check (momentum but not pumped)
            if 10 <= token.price_change_24h <= 500:
                score += 30
                reasons.append(f"Good momentum: +{token.price_change_24h:.1f}%")
            elif token.price_change_24h > 1000:
                reasons.append(f"Already mega-pumped: +{token.price_change_24h:.1f}%")
            elif token.price_change_24h < -50:
                reasons.append(f"Dumping hard: {token.price_change_24h:.1f}%")
            else:
                score += 5
            
            # Age check (catch early but not too early)
            if 12 < token.age_hours < 720:  # 12 hours to 30 days
                score += 20
                reasons.append(f"Good age: {token.age_hours:.1f}h")
            elif token.age_hours < 12:
                score += 10
                reasons.append(f"Very new: {token.age_hours:.1f}h")
            else:
                reasons.append(f"Too old: {token.age_hours:.1f}h")
            
            # Market cap check (wider range for new tokens)
            if 50000 < token.market_cap < 50000000:  # $50k - $50M
                score += 10
                reasons.append(f"Good mcap: ${token.market_cap:,.0f}")
            
            # Keep if score is decent (lowered threshold)
            if score >= 40:
                filtered.append(token)
                self.log(f"âœ“ {token.symbol} passed pre-filter (score: {score}): {', '.join(reasons)}")
            else:
                self.log(f"âœ— {token.symbol} rejected (score: {score}): {', '.join(reasons)}", "DEBUG")
        
        self.log(f"Pre-filter passed: {len(filtered)}/{len(candidates)} tokens")
        return filtered
    
    # ============================================================
    # STAGE 3: CONTRACT SECURITY CHECK
    # ============================================================
    
    def check_contract_security(self, tokens: List[TokenCandidate]) -> List[TokenCandidate]:
        """Stage 3: Check contract security using GoPlus"""
        self.log("STAGE 3: Checking contract security...")
        
        safe_tokens = []
        
        for token in tokens:
            try:
                # Map chain to GoPlus chain ID
                chain_map = {
                    'bsc': '56',
                    'ethereum': '1',
                    'polygon': '137',
                    'arbitrum': '42161',
                    'solana': 'solana'
                }
                
                chain_id = chain_map.get(token.chain.lower(), '56')
                
                # Call GoPlus API
                url = f"{self.goplus_url}/{chain_id}"
                params = {'contract_addresses': token.address}
                
                response = requests.get(url, params=params, timeout=10)
                
                if response.status_code == 200:
                    data = response.json()
                    result = data.get('result', {}).get(token.address.lower(), {})
                    
                    # Analyze security
                    is_safe, reasons = self._analyze_security(result)
                    
                    if is_safe:
                        safe_tokens.append(token)
                        self.log(f"âœ“ {token.symbol} passed security: {', '.join(reasons)}")
                    else:
                        self.log(f"âœ— {token.symbol} FAILED security: {', '.join(reasons)}", "WARNING")
                else:
                    self.log(f"Could not check {token.symbol} security", "WARNING")
                    # Include anyway if API fails
                    safe_tokens.append(token)
                
                time.sleep(0.5)  # Rate limiting
                
            except Exception as e:
                self.log(f"Error checking {token.symbol}: {e}", "ERROR")
                safe_tokens.append(token)  # Include on error
        
        self.log(f"Security check passed: {len(safe_tokens)}/{len(tokens)} tokens")
        return safe_tokens
    
    def _analyze_security(self, security_data: Dict) -> tuple[bool, List[str]]:
        """Analyze GoPlus security data"""
        reasons = []
        is_safe = True
        
        # Check for honeypot
        if security_data.get('is_honeypot') == '1':
            is_safe = False
            reasons.append("HONEYPOT DETECTED")
        
        # Check for mint function
        if security_data.get('is_mintable') == '1':
            is_safe = False
            reasons.append("Mintable (can create tokens)")
        
        # Check ownership
        if security_data.get('owner_change_balance') == '1':
            is_safe = False
            reasons.append("Owner can change balance")
        
        # Check trading cooldown
        if security_data.get('trading_cooldown') == '1':
            reasons.append("Trading cooldown exists")
        
        # Check buy/sell tax
        buy_tax = float(security_data.get('buy_tax', 0))
        sell_tax = float(security_data.get('sell_tax', 0))
        
        if buy_tax > 10 or sell_tax > 10:
            is_safe = False
            reasons.append(f"High tax: Buy {buy_tax}%, Sell {sell_tax}%")
        elif buy_tax > 0 or sell_tax > 0:
            reasons.append(f"Tax: Buy {buy_tax}%, Sell {sell_tax}%")
        
        # Check holder count
        holder_count = int(security_data.get('holder_count', 0))
        if holder_count > 100:
            reasons.append(f"{holder_count} holders")
        elif holder_count < 50:
            is_safe = False
            reasons.append(f"Only {holder_count} holders")
        
        if is_safe and not reasons:
            reasons.append("Clean contract")
        
        return is_safe, reasons
    
    # ============================================================
    # STAGE 4: SOCIAL SENTIMENT (LUNARCRUSH)
    # ============================================================
    
    def analyze_social_sentiment(self, tokens: List[TokenCandidate]) -> List[Dict]:
        """Stage 4: Analyze social sentiment using LunarCrush"""
        self.log("STAGE 4: Analyzing social sentiment...")
        
        tokens_with_sentiment = []
        
        for token in tokens:
            try:
                # LunarCrush API call
                headers = {'Authorization': f'Bearer {self.lunarcrush_key}'}
                url = f"{self.lunarcrush_url}/coins/{token.symbol}/time-series"
                params = {'interval': '1d', 'data_points': 7}
                
                response = requests.get(url, headers=headers, params=params, timeout=10)
                
                sentiment_score = 50  # Default neutral
                social_data = {}
                
                if response.status_code == 200:
                    data = response.json()
                    
                    # Parse LunarCrush data
                    sentiment_score, social_data = self._calculate_sentiment_score(data)
                    self.log(f"{token.symbol} sentiment score: {sentiment_score}/100")
                else:
                    self.log(f"No social data for {token.symbol}", "DEBUG")
                
                tokens_with_sentiment.append({
                    'token': token,
                    'sentiment_score': sentiment_score,
                    'social_data': social_data
                })
                
                time.sleep(0.5)  # Rate limiting
                
            except Exception as e:
                self.log(f"Error getting sentiment for {token.symbol}: {e}", "WARNING")
                tokens_with_sentiment.append({
                    'token': token,
                    'sentiment_score': 50,
                    'social_data': {}
                })
        
        self.log(f"Social sentiment analyzed for {len(tokens_with_sentiment)} tokens")
        return tokens_with_sentiment
    
    def _calculate_sentiment_score(self, lunarcrush_data: Dict) -> tuple[int, Dict]:
        """Calculate sentiment score from LunarCrush data"""
        try:
            timeseries = lunarcrush_data.get('data', [])
            if not timeseries:
                return 50, {}
            
            latest = timeseries[-1] if timeseries else {}
            
            score = 50
            social_data = {}
            
            # Social volume
            social_volume = latest.get('social_volume', 0)
            if social_volume > 100:
                score += 15
                social_data['social_volume'] = social_volume
            
            # Social engagement
            social_engagement = latest.get('social_engagement', 0)
            if social_engagement > 1000:
                score += 15
            
            # Sentiment
            sentiment = latest.get('sentiment', 3)  # 1-5 scale
            if sentiment >= 4:
                score += 20
            elif sentiment <= 2:
                score -= 20
            
            social_data['sentiment'] = sentiment
            
            # Galaxy score (LunarCrush proprietary)
            galaxy_score = latest.get('galaxy_score', 0)
            if galaxy_score > 60:
                score += 10
            
            social_data['galaxy_score'] = galaxy_score
            
            return min(max(score, 0), 100), social_data
            
        except Exception as e:
            return 50, {}
    
    # ============================================================
    # STAGE 5: DEEP HOLDER ANALYSIS (BUBBLEMAPS)
    # ============================================================
    
    def analyze_holder_distribution(self, tokens_with_sentiment: List[Dict]) -> List[Dict]:
        """Stage 5: Deep holder analysis using Bubblemaps (limited quota)"""
        self.log("STAGE 5: Analyzing holder distribution with Bubblemaps...")
        
        final_results = []
        
        # Sort by sentiment score and only analyze top candidates
        sorted_tokens = sorted(tokens_with_sentiment, key=lambda x: x['sentiment_score'], reverse=True)
        
        # Limit to top 15-20 to preserve quota
        tokens_to_analyze = sorted_tokens[:15]
        
        self.log(f"Analyzing top {len(tokens_to_analyze)} candidates with Bubblemaps")
        
        for item in tokens_to_analyze:
            token = item['token']
            
            if not self.check_quota():
                self.log("Bubblemaps quota exhausted - stopping deep analysis", "WARNING")
                break
            
            try:
                # Call Bubblemaps API
                holder_score, holder_data = self._analyze_bubblemaps(token)
                
                # Calculate final score
                final_score = self._calculate_final_score(
                    sentiment_score=item['sentiment_score'],
                    holder_score=holder_score,
                    token=token
                )
                
                result = {
                    'token': token,
                    'final_score': final_score,
                    'sentiment_score': item['sentiment_score'],
                    'holder_score': holder_score,
                    'social_data': item['social_data'],
                    'holder_data': holder_data,
                    'timestamp': datetime.now().isoformat()
                }
                
                final_results.append(result)
                
                self.log(f"âœ“ {token.symbol} ({token.address[:10]}...) Final Score: {final_score}/100 "
                        f"(Sentiment: {item['sentiment_score']}, Holders: {holder_score})")
                
            except Exception as e:
                self.log(f"Error analyzing {token.symbol} with Bubblemaps: {e}", "ERROR")
        
        self.log(f"Deep analysis complete: {len(final_results)} tokens scored")
        return final_results
    
    def _analyze_bubblemaps(self, token: TokenCandidate) -> tuple[int, Dict]:
        """Call Bubblemaps API and analyze holder distribution"""
        try:
            headers = {'X-ApiKey': self.bubblemaps_key}
            
            # Use correct Bubblemaps endpoint and parameters
            url = f"https://api.bubblemaps.io/maps/{token.chain}/{token.address}"
            
            # CRITICAL PARAMETERS - These make Bubblemaps actually useful
            params = {
                'use_magic_nodes': 'true',  # Finds hidden wallet connections
                'return_clusters': 'true',   # Returns grouped wallets
                'return_decentralization_score': 'true',
                'return_nodes': 'true'       # Returns holder data
            }
            
            start_time = time.time()
            response = requests.get(url, headers=headers, params=params, timeout=30)
            elapsed = time.time() - start_time
            
            # Track quota usage
            self.used_quota += elapsed
            self.log(f"Bubblemaps API call took {elapsed:.2f}s (quota used: {self.used_quota:.1f}s)")
            
            if response.status_code == 429:
                self.log("DAILY QUOTA REACHED - Stopping Bubblemaps checks", "ERROR")
                return 50, {}
            
            if response.status_code == 404:
                self.log(f"Bubblemaps map not ready for {token.symbol} (token too new/not indexed)", "WARNING")
                return 50, {}
            
            if response.status_code != 200:
                self.log(f"Bubblemaps API error: {response.status_code}", "WARNING")
                return 50, {}
            
            data = response.json()
            
            # DEBUG: Save raw response for first token to help diagnose
            try:
                debug_file = 'bubblemaps_debug.json'
                if not os.path.exists(debug_file):
                    with open(debug_file, 'w') as f:
                        json.dump({
                            'token': token.symbol,
                            'address': token.address,
                            'chain': token.chain,
                            'response': data
                        }, f, indent=2)
                    self.log(f"   ğŸ’¾ Saved raw Bubblemaps response to {debug_file}", "DEBUG")
            except Exception as debug_err:
                self.log(f"   Could not save debug file: {debug_err}", "DEBUG")
            
            # Parse Bubblemaps data properly
            holder_score, holder_data = self._calculate_holder_score_advanced(data, token)
            
            return holder_score, holder_data
            
        except Exception as e:
            self.log(f"Bubblemaps API error: {e}", "ERROR")
            return 50, {}
    
    def _calculate_holder_score_advanced(self, bubblemaps_data: Dict, token: TokenCandidate) -> tuple[int, Dict]:
        """Advanced holder quality score from Bubblemaps data with cluster analysis"""
        
        # DEBUG: Log the raw response structure
        self.log(f"   ğŸ” DEBUG: Bubblemaps keys: {list(bubblemaps_data.keys())}", "DEBUG")
        
        # Extract nodes (holder wallets)
        raw_nodes = bubblemaps_data.get('nodes', {})
        
        # DEBUG: Check what format nodes are in
        self.log(f"   ğŸ” DEBUG: Nodes type: {type(raw_nodes)}", "DEBUG")
        if isinstance(raw_nodes, dict):
            self.log(f"   ğŸ” DEBUG: Nodes dict keys: {list(raw_nodes.keys())}", "DEBUG")
        
        # Handle both Dict and List formats
        if isinstance(raw_nodes, dict):
            nodes_list = raw_nodes.get('top_holders', [])
        elif isinstance(raw_nodes, list):
            nodes_list = raw_nodes
        else:
            nodes_list = []
        
        self.log(f"   ğŸ” DEBUG: Found {len(nodes_list)} holder nodes", "DEBUG")
        
        if not nodes_list:
            self.log(f"No holder data available for {token.symbol}", "WARNING")
            # Return neutral score when no data
            return 50, {
                'total_holders': 0,
                'defi_score': 0,
                'cluster_count': 0,
                'top_cluster_share': 0,
                'max_single_share': 0,
                'top_10_share': 0,
                'red_flags': ['No holder data available'],
                'green_flags': [],
                'recommendation': {
                    'verdict': 'âš¡ NEUTRAL',
                    'reason': 'No Bubblemaps data available (token too new or not indexed)',
                    'risk_level': 'UNKNOWN',
                    'action': 'Check manually on Bubblemaps website'
                }
            }
        
        # Create lookup map for nodes
        node_map = {}
        for n in nodes_list:
            key = n.get('address') or n.get('id')
            if key:
                node_map[key] = n
        
        # Get decentralization score
        defi_score = bubblemaps_data.get('decentralization_score', 0)
        clusters = bubblemaps_data.get('clusters', [])
        
        self.log(f"   ğŸ” DEBUG: Defi score from API: {defi_score}", "DEBUG")
        self.log(f"   ğŸ” DEBUG: Clusters found: {len(clusters)}", "DEBUG")
        
        # === CLUSTER ANALYSIS (Detect Coordinated Wallets) ===
        top_cluster_share = 0
        cluster_count = len(clusters)
        
        if clusters:
            # Get the largest cluster
            cluster_nodes = clusters[0].get('nodes', [])
            self.log(f"   ğŸ” DEBUG: Top cluster has {len(cluster_nodes)} nodes", "DEBUG")
            
            current_share = 0
            for node_ref in cluster_nodes:
                # If it's a string ID, look it up in node_map
                if isinstance(node_ref, str):
                    wallet_data = node_map.get(node_ref, {})
                    # Get share from holder_data
                    holder_data_obj = wallet_data.get('holder_data', {})
                    share = holder_data_obj.get('share', 0) if holder_data_obj else 0
                    # Try alternatives
                    if share == 0:
                        share = holder_data_obj.get('percentage', 0) if holder_data_obj else 0
                    # Convert if needed
                    if share > 1:
                        share = share / 100
                    current_share += share
                # If it's already an object
                elif isinstance(node_ref, dict):
                    holder_data_obj = node_ref.get('holder_data', {})
                    share = holder_data_obj.get('share', 0) if holder_data_obj else 0
                    if share == 0:
                        share = holder_data_obj.get('percentage', 0) if holder_data_obj else 0
                    if share > 1:
                        share = share / 100
                    current_share += share
            
            top_cluster_share = current_share
        
        # === WHALE ANALYSIS (Detect Single Large Holders) ===
        max_single_share = 0
        top_10_share = 0
        real_holder_count = 0
        
        for i, node in enumerate(nodes_list[:10]):  # Top 10 holders
            # DEBUG: Show what we're seeing in the first node
            if i == 0:
                self.log(f"   ğŸ” DEBUG: First node keys: {list(node.keys())}", "DEBUG")
                holder_data_obj = node.get('holder_data', {})
                self.log(f"   ğŸ” DEBUG: holder_data keys: {list(holder_data_obj.keys()) if holder_data_obj else 'None'}", "DEBUG")
                self.log(f"   ğŸ” DEBUG: holder_data content: {holder_data_obj}", "DEBUG")
            
            # Ignore Contracts, Exchanges, and Liquidity Pools
            details = node.get('address_details', {})
            is_special = (
                node.get('type') in ['contract', 'exchange', 'market_maker', 'liquidity_pool'] or 
                details.get('is_contract') or 
                details.get('is_cex') or 
                details.get('is_dex')
            )
            
            # Get share from holder_data nested object
            holder_data_obj = node.get('holder_data', {})
            share = holder_data_obj.get('share', 0) if holder_data_obj else 0
            
            # If share is still 0, try alternative keys
            if share == 0:
                share = holder_data_obj.get('percentage', 0) if holder_data_obj else 0
            if share == 0:
                share = holder_data_obj.get('percent', 0) if holder_data_obj else 0
            if share == 0:
                share = holder_data_obj.get('balance_percentage', 0) if holder_data_obj else 0
            
            # Convert to decimal if it's a percentage (>1 means it's like 5.3% instead of 0.053)
            if share > 1:
                share = share / 100
            
            if not is_special:
                real_holder_count += 1
                top_10_share += share
                if share > max_single_share:
                    max_single_share = share
        
        self.log(f"   ğŸ” DEBUG: Max single share: {max_single_share}, Top 10 total: {top_10_share}", "DEBUG")
        
        # === SCORING ===
        score = 50  # Start neutral
        red_flags = []
        green_flags = []
        
        # 1. Cluster Risk (Most Important)
        if top_cluster_share > 0.30:  # 30%+ in one group = BAD
            score -= 40
            red_flags.append(f"Coordinated group holds {top_cluster_share*100:.1f}%")
        elif top_cluster_share > 0.20:  # 20-30% = Warning
            score -= 20
            red_flags.append(f"Moderate clustering ({top_cluster_share*100:.1f}%)")
        elif top_cluster_share < 0.10:  # <10% = Good
            score += 25
            green_flags.append(f"Low clustering ({top_cluster_share*100:.1f}%)")
        
        # 2. Single Whale Risk
        if max_single_share > 0.15:  # 15%+ single holder = BAD
            score -= 25
            red_flags.append(f"Large whale holds {max_single_share*100:.1f}%")
        elif max_single_share < 0.05:  # <5% = Good
            score += 15
            green_flags.append(f"No whale risk ({max_single_share*100:.1f}%)")
        
        # 3. Top 10 Concentration
        if top_10_share < 0.25:  # Top 10 < 25% = Excellent
            score += 20
            green_flags.append(f"Well distributed (top 10: {top_10_share*100:.1f}%)")
        elif top_10_share > 0.50:  # Top 10 > 50% = Bad
            score -= 20
            red_flags.append(f"Too concentrated (top 10: {top_10_share*100:.1f}%)")
        
        # 4. Decentralization Score from API
        score += (defi_score - 50) * 0.4
        if defi_score >= 70:
            green_flags.append(f"High defi score ({defi_score}/100)")
        elif defi_score < 40:
            red_flags.append(f"Low defi score ({defi_score}/100)")
        
        # === GENERATE RECOMMENDATION ===
        recommendation = self._generate_recommendation(
            top_cluster_share, max_single_share, top_10_share, defi_score, score
        )
        
        # Log analysis
        self.log(f"   ğŸ“Š Defi Score: {defi_score:.2f}/100")
        self.log(f"   ğŸ”— Cluster Risk: {top_cluster_share*100:.1f}% (Top Cluster)")
        self.log(f"   ğŸ‹ Whale Risk: {max_single_share*100:.1f}% (Largest Holder)")
        self.log(f"   ğŸ“ˆ Top 10: {top_10_share*100:.1f}% of supply")
        self.log(f"   ğŸ¯ RECOMMENDATION: {recommendation['verdict']} - {recommendation['reason']}")
        
        # Build holder data dict
        holder_data = {
            'total_holders': len(nodes_list),
            'defi_score': defi_score,
            'cluster_count': cluster_count,
            'top_cluster_share': round(top_cluster_share * 100, 1),
            'max_single_share': round(max_single_share * 100, 1),
            'top_10_share': round(top_10_share * 100, 1),
            'red_flags': red_flags,
            'green_flags': green_flags,
            'recommendation': recommendation
        }
        
        return min(max(score, 0), 100), holder_data
    
    def _generate_recommendation(self, cluster_share: float, whale_share: float, 
                                 top10_share: float, defi_score: int, holder_score: int) -> Dict:
        """Generate clear BUY/AVOID recommendation"""
        
        # CRITICAL RED FLAGS (Auto-AVOID)
        if cluster_share > 0.30:
            return {
                'verdict': 'ğŸš¨ AVOID',
                'reason': f'Coordinated group controls {cluster_share*100:.1f}% (likely pump & dump)',
                'risk_level': 'CRITICAL',
                'action': 'Do not buy - high scam risk'
            }
        
        if whale_share > 0.20:
            return {
                'verdict': 'ğŸš¨ AVOID',
                'reason': f'Single whale holds {whale_share*100:.1f}% (can dump anytime)',
                'risk_level': 'CRITICAL',
                'action': 'Do not buy - whale dump risk'
            }
        
        # HIGH RISK (Caution)
        if cluster_share > 0.20 or whale_share > 0.15:
            return {
                'verdict': 'âš ï¸ RISKY',
                'reason': f'Centralized holdings (Cluster: {cluster_share*100:.1f}%, Whale: {whale_share*100:.1f}%)',
                'risk_level': 'HIGH',
                'action': 'High risk - only small position if you must'
            }
        
        if top10_share > 0.50:
            return {
                'verdict': 'âš ï¸ RISKY',
                'reason': f'Top 10 holders control {top10_share*100:.1f}% of supply',
                'risk_level': 'HIGH',
                'action': 'High concentration - proceed with extreme caution'
            }
        
        # EXCELLENT (Strong Buy)
        if cluster_share < 0.10 and whale_share < 0.05 and defi_score >= 70:
            return {
                'verdict': 'âœ… SAFE TO BUY',
                'reason': 'Excellent distribution - organic growth pattern',
                'risk_level': 'LOW',
                'action': 'Good entry opportunity - low holder risk'
            }
        
        # GOOD (Buy with normal risk)
        if cluster_share < 0.15 and whale_share < 0.10 and holder_score >= 60:
            return {
                'verdict': 'âœ… BUY',
                'reason': 'Good distribution - acceptable risk level',
                'risk_level': 'MODERATE',
                'action': 'Can buy with standard position size'
            }
        
        # NEUTRAL (Proceed with caution)
        return {
            'verdict': 'âš¡ NEUTRAL',
            'reason': 'Mixed signals - some concerns but not critical',
            'risk_level': 'MODERATE',
            'action': 'Do your own research - moderate risk'
        }
    
    def _calculate_final_score(self, sentiment_score: int, holder_score: int, token: TokenCandidate) -> int:
        """Calculate weighted final score"""
        # Weights
        weights = {
            'sentiment': 0.25,
            'holders': 0.45,  # Most important
            'liquidity': 0.15,
            'momentum': 0.15
        }
        
        # Liquidity score
        liq_score = min(token.liquidity_usd / 10000, 100)  # $1M = 100
        
        # Momentum score
        momentum_score = min(max(token.price_change_24h, 0), 100)
        
        final = (
            sentiment_score * weights['sentiment'] +
            holder_score * weights['holders'] +
            liq_score * weights['liquidity'] +
            momentum_score * weights['momentum']
        )
        
        return int(final)
    
    # ============================================================
    # MAIN SCAN EXECUTION
    # ============================================================
    
    def run_scan(self, chains: List[str] = ['bsc', 'ethereum']) -> List[Dict]:
        """Run complete scanning pipeline"""
        self.log("=" * 60)
        self.log("STARTING TOKEN SCAN")
        self.log("=" * 60)
        
        start_time = time.time()
        
        # Stage 1: Discover
        candidates = self.discover_new_tokens(chains)
        
        if not candidates:
            self.log("No candidates found", "WARNING")
            return []
        
        # Stage 2: Pre-filter
        filtered = self.pre_filter_tokens(candidates)
        
        if not filtered:
            self.log("No tokens passed pre-filter", "WARNING")
            return []
        
        # Stage 3: Security check
        safe_tokens = self.check_contract_security(filtered)
        
        if not safe_tokens:
            self.log("No tokens passed security check", "WARNING")
            return []
        
        # Stage 4: Social sentiment
        tokens_with_sentiment = self.analyze_social_sentiment(safe_tokens)
        
        # Stage 5: Deep holder analysis
        final_results = self.analyze_holder_distribution(tokens_with_sentiment)
        
        # Sort by final score
        final_results.sort(key=lambda x: x['final_score'], reverse=True)
        
        # Log summary
        elapsed = time.time() - start_time
        self.log("=" * 60)
        self.log(f"SCAN COMPLETE in {elapsed:.1f}s")
        self.log(f"Found {len(final_results)} promising tokens")
        self.log("=" * 60)
        
        # Store results
        self.results = final_results
        self.scan_history.append({
            'timestamp': datetime.now().isoformat(),
            'tokens_found': len(final_results),
            'elapsed_seconds': elapsed
        })
        
        return final_results
    
    # ============================================================
    # RESULTS EXPORT
    # ============================================================
    
    def print_results(self, top_n: int = 10):
        """Print formatted results"""
        if not self.results:
            print("No results to display")
            return
        
        print("\n" + "=" * 80)
        print(f"TOP {min(top_n, len(self.results))} TOKEN OPPORTUNITIES")
        print("=" * 80)
        
        for i, result in enumerate(self.results[:top_n], 1):
            token = result['token']
            hd = result.get('holder_data', {})
            rec = hd.get('recommendation', {})
            
            print(f"\n#{i} - {token.symbol} ({token.name})")
            print(f"   ğŸ“ Address: {token.address}")
            print(f"   â›“ï¸  Chain: {token.chain}")
            
            # === MAIN RECOMMENDATION (Big and Clear) ===
            if rec:
                print(f"\n   â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—")
                print(f"   â•‘  {rec.get('verdict', 'N/A'):^53} â•‘")
                print(f"   â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£")
                print(f"   â•‘  {rec.get('reason', 'N/A'):51} â•‘")
                print(f"   â•‘  ACTION: {rec.get('action', 'N/A'):43} â•‘")
                print(f"   â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
            
            print(f"\n   ğŸ¯ FINAL SCORE: {result['final_score']}/100")
            print(f"   ğŸ’° Price: ${token.price_usd:.8f} | 24h: {token.price_change_24h:+.1f}%")
            print(f"   ğŸ’§ Liquidity: ${token.liquidity_usd:,.0f} | Volume: ${token.volume_24h:,.0f}")
            print(f"   ğŸ“Š Market Cap: ${token.market_cap:,.0f}")
            print(f"   ğŸ—£ï¸  Social Sentiment: {result['sentiment_score']}/100")
            print(f"   ğŸ‘¥ Holder Score: {result['holder_score']}/100")
            
            if hd:
                print(f"\n   ğŸ“ˆ HOLDER ANALYSIS:")
                print(f"      â€¢ Total Holders: {hd.get('total_holders', 'N/A')}")
                print(f"      â€¢ Defi Score: {hd.get('defi_score', 'N/A')}/100")
                print(f"      â€¢ ğŸ”— Top Cluster: {hd.get('top_cluster_share', 'N/A')}% ({hd.get('cluster_count', 0)} clusters)")
                print(f"      â€¢ ğŸ‹ Largest Whale: {hd.get('max_single_share', 'N/A')}%")
                print(f"      â€¢ ğŸ“Š Top 10 Holdings: {hd.get('top_10_share', 'N/A')}%")
                
                # Show green/red flags
                green_flags = hd.get('green_flags', [])
                red_flags = hd.get('red_flags', [])
                
                if green_flags:
                    print(f"\n   âœ… POSITIVE SIGNS:")
                    for flag in green_flags:
                        print(f"      â€¢ {flag}")
                
                if red_flags:
                    print(f"\n   ğŸš© WARNING SIGNS:")
                    for flag in red_flags:
                        print(f"      â€¢ {flag}")
            
            print(f"\n   ğŸ”— DexScreener: https://dexscreener.com/{token.chain}/{token.pair_address}")
            print(f"   ğŸ”— Bubblemaps: https://app.bubblemaps.io/{token.chain}/token/{token.address}")
    
    def export_to_csv(self, filename: str = "token_scan_results.csv"):
        """Export results to CSV"""
        if not self.results:
            self.log("No results to export", "WARNING")
            return
        
        rows = []
        for result in self.results:
            token = result['token']
            row = {
                'timestamp': result['timestamp'],
                'symbol': token.symbol,
                'name': token.name,
                'chain': token.chain,
                'address': token.address,
                'final_score': result['final_score'],
                'sentiment_score': result['sentiment_score'],
                'holder_score': result['holder_score'],
                'price_usd': token.price_usd,
                'price_change_24h': token.price_change_24h,
                'liquidity_usd': token.liquidity_usd,
                'volume_24h': token.volume_24h,
                'market_cap': token.market_cap,
                'dexscreener_link': f"https://dexscreener.com/{token.chain}/{token.pair_address}"
            }
            rows.append(row)
        
        df = pd.DataFrame(rows)
        df.to_csv(filename, index=False)
        self.log(f"Results exported to {filename}")
    
    def export_to_json(self, filename: str = "token_scan_results.json"):
        """Export detailed results to JSON"""
        if not self.results:
            self.log("No results to export", "WARNING")
            return
        
        export_data = {
            'scan_timestamp': datetime.now().isoformat(),
            'quota_used': self.used_quota,
            'tokens_analyzed': len(self.results),
            'results': []
        }
        
        for result in self.results:
            token = result['token']
            export_data['results'].append({
                'token': {
                    'symbol': token.symbol,
                    'name': token.name,
                    'chain': token.chain,
                    'address': token.address,
                    'price_usd': token.price_usd,
                    'liquidity_usd': token.liquidity_usd,
                    'volume_24h': token.volume_24h,
                    'price_change_24h': token.price_change_24h,
                    'market_cap': token.market_cap,
                    'pair_address': token.pair_address
                },
                'scores': {
                    'final_score': result['final_score'],
                    'sentiment_score': result['sentiment_score'],
                    'holder_score': result['holder_score']
                },
                'social_data': result['social_data'],
                'holder_data': result['holder_data'],
                'links': {
                    'dexscreener': f"https://dexscreener.com/{token.chain}/{token.pair_address}",
                    'bubblemaps': f"https://app.bubblemaps.io/{token.chain}/token/{token.address}"
                }
            })
        
        with open(filename, 'w') as f:
            json.dump(export_data, f, indent=2)
        
        self.log(f"Detailed results exported to {filename}")


# ============================================================
# MAIN EXECUTION
# ============================================================

def main():
    """Main execution function"""
    
    # Get API keys from environment or hardcode (use .env file in production)
    BUBBLEMAPS_KEY = os.getenv('BUBBLEMAPS_API_KEY', '')
    LUNARCRUSH_KEY = os.getenv('LUNARCRUSH_API_KEY', '')
    
    # Initialize scanner
    scanner = TokenScanner(
        bubblemaps_key=BUBBLEMAPS_KEY,
        lunarcrush_key=LUNARCRUSH_KEY
    )
    
    # Run scan
    chains_to_scan = ['bsc', 'ethereum']  # Add more chains as needed
    results = scanner.run_scan(chains=chains_to_scan)
    
    # Display results
    scanner.print_results(top_n=10)
    
    # Export results
    scanner.export_to_csv('token_opportunities.csv')
    scanner.export_to_json('token_opportunities.json')
    
    print(f"\nâœ… Scan complete! Found {len(results)} opportunities")
    print(f"ğŸ“Š Results saved to token_opportunities.csv and .json")


if __name__ == "__main__":
    main()
